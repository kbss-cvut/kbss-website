---
title: "Upcoming Open Mic - LLM-Powered Coder Assistants"
categories: [Open Mic Announcement, Open Mic]
---


The next Open Mic session starts on Friday 25th April 2025 at 9:30
at this link. Evgenii Grigorev will unpack how Large Language Models (LLMs)
like GPT-4 and CodeLlama generate code, blending theory with real-world examples.

##### Abstract

Code-generating LLMs are not wizards — they’re sophisticated pattern matchers trained on terabytes of code. But how do they turn a prompt like "Sort this CSV by date and calculate weekly averages" into working Python? This session will demystify:

1) Core mechanics: Transformers, attention layers, and tokenization.

2) Training secrets: From GitHub scrapes to context-aware fine-tuning.

3) Why they fail: Hallucinations, hidden biases, and the "copy-paste paradox".

4) Examples from data analysis (Pandas, SQL) will illustrate key concepts.

**Outline**

1) Introduction to LLMs: Transformers, tokenization, and the "autocomplete on steroids" paradigm.

2) Tools Deep Dive: GitHub Copilot, ChatGPT, CodeWhisperer, and open-source alternatives (StarCoder, Llama 3).

3) Under the Hood: Training on GitHub data, context window limitations, and safety guardrails.

4) Pros vs. Cons: 55% faster coding (GitHub study) vs. 40% of generated code containing vulnerabilities (Stanford research).

Further reading:
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)